
<!DOCTYPE html>
<html>

<head>


<link rel="stylesheet" href="style.css">


</head>


<body onload="refresh()">


  <div class="topnav">
    <h1> Research </h1>
  </div>
  <div class="sidenav">
    <img src="images/nyc_central_park_pic_400_square.jpg"
     alt="Pic of me or smth">
     <h1> James Enouen</h1>
     <h2> enouen@usc.edu </h2>

    <a href="..">Home</a>
    <a href="" class="active">Research</a>
    <a href="../cv">Curriculum Vitae</a>
    <a href="../coursework">Undergraduate Coursework</a>
    <a href="../projects">Projects</a>
    <a href="../blog">Blog</a>
    <a href="../other">Other</a>
  </div>

  <div class="main">

<div class="gap"></div>
<div class="gap">
  <h3>
  <u>Research Statement</u>
</h3>
<p>
<br>
I want to further investigate <span style="font-weight:bold">deep learning </span>
from a statistical machine
learning perspective.  I want to better understand:
<ul>
  <li>
The <span style="font-weight:bold">dynamics of training</span> while using gradient
descent methods on deep networks.
</li>
  <li>
The <span style="font-weight:bold">representative power</span> of networks using
 particular architectures and specific datasets.
</li>
<li>
   Creating reasonable <span style="font-weight:bold"> interpretations </span> of decisions and predictions
   made by machine learning models for both field experts and novices.
</li>
</ul>
</p>
<p>
In particular, I want to assess the interplay between training and validating and how the assumptions
we originally make align with the studied regimes of neural networks.
I believe these questions are fundamental to every nonconvex model we use and this path is key
to unlocking new potential of deep models' already incredible functionality.
Because I am interested in deep learning at such a fundamental level, I am interested in a
very broad array of application fields ranging across computer vision, climate science, computational biology, and deep reinforcement learning.
I am especially interested in methodologies which incorporate the structure of the data into the structure of the model
(e.g. convolutional networks, phsyics-informed machine learning, graph neural networks, etc.)

<br>
</p>

<div class="gap"></div>

<h3>
<u>Current Directions</u>
</h3>
<p>
One of the main things I am currently studying are "Feature Interactions" which
I believe to be one of the best statistical frameworks to study interpretability
 for deep learning.
A feature interaction broadly describes a situation where two features are <i>simultaneously</i>
important for a prediction (e.g. "not" and "good" for the sentiment of "that movie was not good.")
  In my opinion, this situation describes the <i>vast</i>
majority of real world data.
This framework rigorously formalizes a nonadditive interaction and lends
itself to a new (and in my opinion better) way of explaining the decisions made
by deep neural networks.
Consequently, there are many research directions emerging from this field for better machine
learning interpretation methods, redesigning neural network architectures for better performance, and
new insights for modelling epistasis in gene sequencing.
One of the ways I am currently applying this is using randomized experiment data to evaluate
the impact and fairness of a learned model.
</p>

<img src="images/fi_gradient_expectation.PNG" alt="FI Gradeint Definition"
height="91" width="202" style="margin-left:69px" border="2"></img>
<img src="images/cool_plot.PNG" alt="FI causal inference plot"
height="181" width="330" style="margin-left:69px" border="2"></img>


<div class="gap"></div>

  <h3>
<u>Previous Publications</u>
</h3>
<p>
<br>
<a>
  Hierarchical Classification with Confidence using Generalized Logits
</a> (ICPR-2021)
<br>
<a href="papers/isvc19a.pdf">
Hierarchical Semantic Labeling with Adaptive Confidence
</a> (ISVC-2019)
<br>
<img src="images/hierarchical_figure.PNG" alt="Figure of Main Idea"
height="181" width="462" style="margin-left:69px" border="2"></img>
<br>
</p>

<h3>
<u>Combinatorics Research Projects</u>
</h3>
<p>
<a href="https://arxiv.org/abs/1911.07426"> The Perfect Shuffle</a> (Combinatorics) (Summer 2019)
<br>
<a href="https://people.math.osu.edu/chmutov.1/wor-gr-su19/Rushil-Raghavan-MIGHTY_10192019.pdf">
Signed Symmetric Chromatic Polynomial</a> (Graph Theory) (Summer 2018)
<br>
</p>
</div>

</div>
</body>
</html>
